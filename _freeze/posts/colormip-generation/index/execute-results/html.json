{
  "hash": "dd0e5c333537256a8d0d3bf6bf40539f",
  "result": {
    "markdown": "---\ntitle: Create ColorMIPs for Segmented EM Neurons\nauthor: Stephan Gerhard\ndate: '2023-12-18'\nimage: image.jpg\ncategories:\n  - visualization\ndraft: true\nexecute:\n  enabled: true\n  freeze: auto\n---\n\n## Goals\n\nWe demonstrate how to generate so-called ColorMIP images from 3D meshes of segmented neurons in EM datasets. ColorMIPs are a way to represent a complex 3d structure as 2d images by encoding depth and signal strength information. They were introduced as part of the [NeuronBridge](https://neuronbridge.janelia.org/about) project at Janelia for finding genetic lines with relevant expression patterns.\n\n## Datasets\n\nWe are going to map EM neurons for fruit fly's brain and VNC from the FlyWire and FANC datasets.\n\n## General Approach\n\nWe are going to map EM neuron meshes, transform them into common unisex template spaces. We then use the vertex coordinates to label voxels in the 3d template space belonging to the neuron, and then apply a transformation using a common color-lookup table to project z coordinates of the neuron's morphology to a colored 2d image, i.e. the Color Depth Maximum Intensity Projection, or ColorMIP.\n\n## Steps\n\nFirst, we start with the brain. We need to load an example segment using [CloudVolume](https://github.com/seung-lab/cloud-volume) from a recent published FlyWire release.\n\n```python\nsegment_id = 720575940601206499\n\nfrom cloudvolume import CloudVolume\n\nvol = CloudVolume('precomputed://gs://flywire_v141_m630', progress=False, use_https=True)\nm = vol.mesh.get(segment_id)[segment_id]\n```\n\nIn the next step, we convert the mesh vertices to a target template space using the [NAVis FlyBrains package](https://github.com/navis-org/navis-flybrains/).\n\n```python\nimport flybrains\nvoxel_size = (0.5189161, 0.5189161, 1.0)\n\nvertices = navis.xform_brain(m.vertices, source='FLYWIRE', target='FAFB14', via='FAFB14raw')\nvertices2 = navis.xform_brain(vertices, source='FAFB14', target='JRC2018U')\nvertices3 = vertices2 / voxel_size\n```\n\nThen, we use the `neuron2mip` function to transform the mesh into an image, and store the image.\n\n```python\nresult_image = neuron2mip(vertices3, 'brain')\nio.imsave(f'{segment_id}.png', result_image)\n```\n\nHere is how the original neuron looks like in 3D, and as a ColorMIP image.\n\nIMAGE\n\nLet's dive deeper into the `neuron2mip` function.\n\n```python\nfrom .helper import lutmap\n\ndef neuron2mip(mesh_vertices, target_space):\n\n    \n    if target_space == 'brain':\n        # JRC2018_BRAIN_UNISEX\n        # different version rescaled for the neuronbridge data\n        # https://open.quiltdata.com/b/janelia-flylight-color-depth/tree/Color_Depth_MIPs_For_Download/JRC2018_UNISEX_20x_HR.nrrd\n        dim = (1210,  566,  174)\n        voxel_size = (0.5189161, 0.5189161, 1.0)\n\n    elif target_space == 'vnc':\n        # JRC2018_VNC_UNISEX_461\n        # added 90 pixel on top for colorbar placeholder\n        dim = (573, 1119, 219)\n        voxel_size = (0.461122, 0.461122, 0.7)\n    elif:\n        raise Exception('Unknown target space')\n\n    maxzval = dim[2]-1   \n    rendered_image = np.zeros(dim, dtype=np.uint8)\n    m = mesh_vertices.astype(np.uint32)\n    rendered_image[m[:, 0], m[:, 1], m[:, 2]] = 255\n    b = np.argmax(rendered_image, axis=2).astype(np.float64)\n    r = ((b/maxzval) * 255).astype(np.uint32)\n    b = b.astype(np.int32)\n    outputcol = np.zeros((rendered_image.shape[0], rendered_image.shape[1], 3), dtype=np.uint8)\n    for i in range(r.shape[0]):\n        for j in range(r.shape[1]):\n            if r[i,j] != 0:\n                hsv = rgb2hsv(np.array(lutmap[r[i,j]], dtype=np.double))\n                hsv[2] = 255\n                outputcol[i,j,:] = hsv2rgb(hsv).astype(np.uint8)\n    \n    return outputcol.transpose(1,0,2).astype(np.uint8)\n\n```\n\nNow, let's generate a ColorMIP for a VNC neuron based on the FANC dataset.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}